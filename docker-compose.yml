version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: deepseek_ollama
    # Ollama API port internally
    expose:
      - "11434"
    volumes:
      - ./ollama_data:/root/.ollama
    networks:
      - applygenie_network
    restart: unless-stopped
    # Use GPU if available on the RunPod instance
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  applygenie:
    build: .
    container_name: applygenie_agent
    environment:
      # We connect back to the ollama service on the custom network
      - OLLAMA_BASE_URL=http://ollama:11434
      # Define resolutions and settings
      - VNC_RESOLUTION=1920x1080x24
      - DISPLAY=:99
      # Password for the VNC connection (Set securely via ENV variables for RunPod)
      - VNC_PASSWORD=${VNC_PASSWORD:-applygenie2026}
      # Streamlit configuration
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
    ports:
      # Expose noVNC for visual monitoring
      - "8080:8080"
      # Expose Streamlit for the dashboard UI
      - "8501:8501"
      # Expose FastAPI for any programmatic access/webhooks
      - "8000:8000"
    volumes:
      # Mount the app code so we can develop live without rebuilding Docker if we want
      - .:/app
    depends_on:
      - ollama
    networks:
      - applygenie_network
    restart: unless-stopped
    # Share memory so Chrome has enough RAM for rendering large sites
    shm_size: "2gb"

networks:
  applygenie_network:
    driver: bridge

volumes:
  ollama_data:
